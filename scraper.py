"""
–ú–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
    - habr.com (—Å—Ç–∞—Ç—å–∏)
    - github.com (README —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤)

Example:
    >>> from scraper import get_article
    >>> data = get_article('https://habr.com/ru/articles/123456/')
    >>> print(data['title'])
"""

import re

import requests
from bs4 import BeautifulSoup

# –ü—É–±–ª–∏—á–Ω—ã–π API –º–æ–¥—É–ª—è
__all__ = ['get_article', 'get_structured_habr_article']

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_CONTENT_LENGTH: int = 8000
TIMEOUT_SECONDS: int = 10
USER_AGENT: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'


def get_article(url: str) -> dict:
    """
    –†–æ—É—Ç–µ—Ä: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä—Å–µ—Ä.

    Args:
        url: URL —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏. –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è:
            - url: –∏—Å—Ö–æ–¥–Ω—ã–π URL
            - source: –∏—Å—Ç–æ—á–Ω–∏–∫ ('habr' | 'github')
            - title: –∑–∞–≥–æ–ª–æ–≤–æ–∫
            - author: –∞–≤—Ç–æ—Ä
            - content: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            - content_length: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
        
        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –ø–æ–ª–µ–º:
            - error: –æ–ø–∏—Å–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
    """
    url = url.strip()

    if 'habr.com' in url:
        return _parse_habr(url)
    elif 'github.com' in url:
        return _parse_github(url)
    else:
        return {'error': f'–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {url}'}


def _fetch_page(url: str) -> tuple[BeautifulSoup | None, dict | None]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.

    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
            - BeautifulSoup –æ–±—ä–µ–∫—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
            - None –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'error'
    """
    headers = {'User-Agent': USER_AGENT}

    try:
        response = requests.get(url, headers=headers, timeout=TIMEOUT_SECONDS)
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser'), None

    except requests.exceptions.Timeout:
        return None, {'error': '–¢–∞–π–º–∞—É—Ç: —Å–∞–π—Ç –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ 10 —Å–µ–∫—É–Ω–¥'}
    except requests.exceptions.HTTPError as e:
        return None, {'error': f'HTTP {e.response.status_code}: {e.response.reason}'}
    except requests.exceptions.RequestException as e:
        return None, {'error': f'–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {str(e)}'}


def _truncate_content(text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é MAX_CONTENT_LENGTH).

    Returns:
        –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å '...' –≤ –∫–æ–Ω—Ü–µ –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    if len(text) > max_length:
        return text[:max_length] + '...'
    return text


# =============================================================================
# –ü–∞—Ä—Å–µ—Ä –•–∞–±—Ä–∞
# =============================================================================


def _parse_habr(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å—é —Å –•–∞–±—Ä–∞.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∞–≤—Ç–æ—Ä, –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏.

    Args:
        url: URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ –•–∞–±—Ä–µ.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏: url, source, title, author, date, content, content_length.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–µ–º 'error'.
    """
    print(f'üì• –ó–∞–≥—Ä—É–∂–∞—é –•–∞–±—Ä: {url}')

    soup, error = _fetch_page(url)
    if error:
        return error

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_elem = soup.find('h1', class_='tm-title')
    title = title_elem.get_text(strip=True) if title_elem else '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –ê–≤—Ç–æ—Ä
    author = _extract_habr_author(soup)

    # –î–∞—Ç–∞
    date_elem = soup.find('span', class_='tm-article-datetime-published')
    date = date_elem.get_text(strip=True) if date_elem else '–ù–µ –Ω–∞–π–¥–µ–Ω–∞'

    # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    content = _extract_habr_content(soup)

    return {
        'url': url,
        'source': 'habr',
        'title': title,
        'author': author,
        'date': date,
        'content': content,
        'content_length': len(content),
    }


def _extract_habr_author(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—å–∏ —Å –•–∞–±—Ä–∞."""
    author_elem = soup.find('a', class_='tm-user-info__username')
    if author_elem:
        author_span = author_elem.find('span')
        return author_span.get_text(strip=True) if author_span else author_elem.get_text(strip=True)
    return '–ù–µ –Ω–∞–π–¥–µ–Ω'


def _extract_habr_content(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –•–∞–±—Ä–∞."""
    content_elem = soup.find('div', id='post-content-body')
    if not content_elem:
        return '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
    for tag in content_elem(['script', 'style', 'aside']):
        tag.decompose()

    content = content_elem.get_text(separator='\n', strip=True)
    return _truncate_content(content)


# =============================================================================
# –ü–∞—Ä—Å–µ—Ä GitHub
# =============================================================================


def _parse_github(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç README —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å GitHub.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ, –≤–ª–∞–¥–µ–ª–µ—Ü, –æ–ø–∏—Å–∞–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ README,
    –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–≤—ë–∑–¥, –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫.

    Args:
        url: URL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ GitHub.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏: url, source, title, author, description,
        stars, language, content, content_length.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–µ–º 'error'.
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º owner/repo –∏–∑ URL
    match = re.search(r'github\.com/([^/]+)/([^/]+)', url)
    if not match:
        return {'error': '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL GitHub. –û–∂–∏–¥–∞–µ—Ç—Å—è: github.com/owner/repo'}

    owner, repo = match.groups()
    repo = repo.rstrip('/')

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π URL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
    repo_url = f'https://github.com/{owner}/{repo}'
    print(f'üì• –ó–∞–≥—Ä—É–∂–∞—é GitHub: {repo_url}')

    soup, error = _fetch_page(repo_url)
    if error:
        return error

    # –û–ø–∏—Å–∞–Ω–∏–µ (–∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º)
    description = ''
    desc_elem = soup.find('p', class_='f4')
    if desc_elem:
        description = desc_elem.get_text(strip=True)

    return {
        'url': repo_url,
        'source': 'github',
        'title': f'{owner}/{repo}',
        'author': owner,
        'description': description,
        'stars': _extract_github_stars(soup),
        'language': _extract_github_language(soup),
        'content': _extract_github_readme(soup),
        'content_length': len(_extract_github_readme(soup)),
    }


def _extract_github_stars(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–≤—ë–∑–¥ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è."""
    star_elem = soup.find('a', href=lambda x: x and '/stargazers' in x)
    if star_elem:
        star_text = star_elem.get_text(strip=True)
        numbers = re.findall(r'[\d,.]+[kK]?', star_text)
        if numbers:
            return numbers[0]
    return '0'


def _extract_github_language(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."""
    lang_elem = soup.find(
        'span',
        class_='color-fg-default',
        attrs={'itemprop': 'programmingLanguage'},
    )
    if lang_elem:
        return lang_elem.get_text(strip=True)
    return '–ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω'


def _extract_github_readme(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ README."""
    readme_elem = soup.find('article', class_='markdown-body')

    if not readme_elem:
        return 'README –Ω–µ –Ω–∞–π–¥–µ–Ω'

    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    for tag in readme_elem(['script', 'style', 'svg', 'img']):
        tag.decompose()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    content = readme_elem.get_text(separator='\n', strip=True)

    # –û—á–∏—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    content = re.sub(r'\n{3,}', '\n\n', content)

    return _truncate_content(content)


# =============================================================================
# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# =============================================================================


def _run_tests() -> None:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –ø–∞—Ä—Å–µ—Ä–æ–≤."""
    test_urls = [
        'https://habr.com/ru/articles/984968/',
        'https://github.com/anthropics/anthropic-cookbook',
    ]

    for url in test_urls:
        print(f'\n{"=" * 60}')
        print(f'–¢–ï–°–¢: {url}')
        print('=' * 60)

        result = get_article(url)

        if 'error' in result:
            print(f'‚ùå –û—à–∏–±–∫–∞: {result["error"]}')
        else:
            print(f'‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫: {result["source"]}')
            print(f'   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {result["title"]}')
            print(f'   –ê–≤—Ç–æ—Ä: {result["author"]}')
            print(f'   –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result["content_length"]} —Å–∏–º–≤–æ–ª–æ–≤')
            print(f'\n   –ü—Ä–µ–≤—å—é (200 —Å–∏–º–≤–æ–ª–æ–≤):')
            print(f'   {result["content"][:200]}...')


if __name__ == '__main__':
    _run_tests()