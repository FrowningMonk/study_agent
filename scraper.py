"""
–ú–æ–¥—É–ª—å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:
    - habr.com (—Å—Ç–∞—Ç—å–∏)
    - github.com (README + –¥—Ä—É–≥–∏–µ .md —Ñ–∞–π–ª—ã: ARCHITECTURE.md, CONTRIBUTING.md, docs/ –∏ —Ç.–¥.)
    - infostart.ru (—Å—Ç–∞—Ç—å–∏ –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –ø–æ 1–°)

Example:
    >>> from scraper import get_article
    >>> data = get_article('https://habr.com/ru/articles/123456/')
    >>> print(data['title'])
    >>> data = get_article('https://github.com/owner/repo')
    >>> print(data['files'])  # –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö markdown —Ñ–∞–π–ª–æ–≤
    >>> data = get_article('https://infostart.ru/1c/articles/123456/')
    >>> print(data['title'])
"""

import re

import requests
from bs4 import BeautifulSoup

# –ü—É–±–ª–∏—á–Ω—ã–π API –º–æ–¥—É–ª—è
__all__ = ['get_article', 'get_structured_habr_article']

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_CONTENT_LENGTH: int = 8000
TIMEOUT_SECONDS: int = 10
USER_AGENT: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'


def get_article(url: str) -> dict:
    """
    –†–æ—É—Ç–µ—Ä: –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä—Å–µ—Ä.

    Args:
        url: URL —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—Ç–∞—Ç—å–∏. –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–ª—è:
            - url: –∏—Å—Ö–æ–¥–Ω—ã–π URL
            - source: –∏—Å—Ç–æ—á–Ω–∏–∫ ('habr' | 'github' | 'infostart')
            - title: –∑–∞–≥–æ–ª–æ–≤–æ–∫
            - author: –∞–≤—Ç–æ—Ä
            - content: –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç
            - content_length: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞

        –ü—Ä–∏ –æ—à–∏–±–∫–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –ø–æ–ª–µ–º:
            - error: –æ–ø–∏—Å–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
    """
    url = url.strip()

    if 'habr.com' in url:
        return _parse_habr(url)
    elif 'github.com' in url:
        return _parse_github(url)
    elif 'infostart.ru' in url:
        return _parse_infostart(url)
    else:
        return {'error': f'–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {url}'}


def _fetch_page(url: str) -> tuple[BeautifulSoup | None, dict | None]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.

    Args:
        url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏.

    Returns:
        –ö–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
            - BeautifulSoup –æ–±—ä–µ–∫—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
            - None –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–æ–º 'error'
    """
    headers = {'User-Agent': USER_AGENT}

    try:
        response = requests.get(url, headers=headers, timeout=TIMEOUT_SECONDS)
        response.raise_for_status()
        return BeautifulSoup(response.text, 'html.parser'), None

    except requests.exceptions.Timeout:
        return None, {'error': '–¢–∞–π–º–∞—É—Ç: —Å–∞–π—Ç –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ 10 —Å–µ–∫—É–Ω–¥'}
    except requests.exceptions.HTTPError as e:
        return None, {'error': f'HTTP {e.response.status_code}: {e.response.reason}'}
    except requests.exceptions.RequestException as e:
        return None, {'error': f'–û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {str(e)}'}


def _truncate_content(text: str, max_length: int = MAX_CONTENT_LENGTH) -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é MAX_CONTENT_LENGTH).

    Returns:
        –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å '...' –≤ –∫–æ–Ω—Ü–µ –∏–ª–∏ –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    if len(text) > max_length:
        return text[:max_length] + '...'
    return text


# =============================================================================
# –ü–∞—Ä—Å–µ—Ä –•–∞–±—Ä–∞
# =============================================================================


def _parse_habr(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å—é —Å –•–∞–±—Ä–∞.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∞–≤—Ç–æ—Ä, –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏.

    Args:
        url: URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ –•–∞–±—Ä–µ.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏: url, source, title, author, date, content, content_length.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–µ–º 'error'.
    """
    print(f'üì• –ó–∞–≥—Ä—É–∂–∞—é –•–∞–±—Ä: {url}')

    soup, error = _fetch_page(url)
    if error:
        return error

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_elem = soup.find('h1', class_='tm-title')
    title = title_elem.get_text(strip=True) if title_elem else '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –ê–≤—Ç–æ—Ä
    author = _extract_habr_author(soup)

    # –î–∞—Ç–∞
    date_elem = soup.find('span', class_='tm-article-datetime-published')
    date = date_elem.get_text(strip=True) if date_elem else '–ù–µ –Ω–∞–π–¥–µ–Ω–∞'

    # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    content = _extract_habr_content(soup)

    return {
        'url': url,
        'source': 'habr',
        'title': title,
        'author': author,
        'date': date,
        'content': content,
        'content_length': len(content),
    }


def _extract_habr_author(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è –∞–≤—Ç–æ—Ä–∞ —Å—Ç–∞—Ç—å–∏ —Å –•–∞–±—Ä–∞."""
    author_elem = soup.find('a', class_='tm-user-info__username')
    if author_elem:
        author_span = author_elem.find('span')
        return author_span.get_text(strip=True) if author_span else author_elem.get_text(strip=True)
    return '–ù–µ –Ω–∞–π–¥–µ–Ω'


def _extract_habr_content(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å –•–∞–±—Ä–∞."""
    content_elem = soup.find('div', id='post-content-body')
    if not content_elem:
        return '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
    for tag in content_elem(['script', 'style', 'aside']):
        tag.decompose()

    content = content_elem.get_text(separator='\n', strip=True)
    return _truncate_content(content)


# =============================================================================
# –ü–∞—Ä—Å–µ—Ä InfoStart
# =============================================================================


def _parse_infostart(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç–∞—Ç—å—é —Å InfoStart.ru.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏. –ê–≤—Ç–æ—Ä –∏ –¥–∞—Ç–∞ —á–∞—Å—Ç–æ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã
    –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

    Args:
        url: URL —Å—Ç–∞—Ç—å–∏ –Ω–∞ InfoStart.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏: url, source, title, author, content, content_length.
        –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–µ–º 'error'.
    """
    print(f'üì• –ó–∞–≥—Ä—É–∂–∞—é InfoStart: {url}')

    soup, error = _fetch_page(url)
    if error:
        return error

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    title_elem = soup.find('h1', class_='main-title')
    title = title_elem.get_text(strip=True) if title_elem else '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –ê–≤—Ç–æ—Ä - –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –Ω–∞ –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    author = '–ù–µ –Ω–∞–π–¥–µ–Ω'
    author_elem = soup.find('a', href=lambda x: x and '/users/' in x)
    if author_elem:
        author = author_elem.get_text(strip=True)

    # –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
    content = _extract_infostart_content(soup)

    return {
        'url': url,
        'source': 'infostart',
        'title': title,
        'author': author,
        'content': content,
        'content_length': len(content),
    }


def _extract_infostart_content(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å InfoStart."""
    # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ div.kurs-spoiler –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    content_elem = soup.find('div', class_='kurs-spoiler')

    if not content_elem:
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –∏—â–µ–º –≤ div.public-text-wrapper
        content_elem = soup.find('div', class_='public-text-wrapper')

    if not content_elem:
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –±–µ—Ä–µ–º div.content
        content_elem = soup.find('div', class_='content')

    if not content_elem:
        return '–ù–µ –Ω–∞–π–¥–µ–Ω'

    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
    for tag in content_elem(['script', 'style', 'aside', 'iframe', 'nav']):
        tag.decompose()

    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –±–ª–æ–∫–∏
    for elem in content_elem.find_all('div', class_=['forum-message-wrap', 'comments', 'comment']):
        elem.decompose()

    content = content_elem.get_text(separator='\n', strip=True)

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ—Å—Ç–æ—è—â–∏–µ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤ > (–Ω–∞–≤–∏–≥–∞—Ü–∏—è)
    lines = content.split('\n')
    cleaned_lines = [line for line in lines if line.strip() and line.strip() != '>']
    content = '\n'.join(cleaned_lines)

    # –û—á–∏—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    content = re.sub(r'\n{3,}', '\n\n', content)

    return _truncate_content(content)


# =============================================================================
# –ü–∞—Ä—Å–µ—Ä GitHub
# =============================================================================

# –°–ø–∏—Å–æ–∫ –≤–∞–∂–Ω—ã—Ö markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
IMPORTANT_MD_FILES = [
    'README.md',
    'ARCHITECTURE.md',
    'CONTRIBUTING.md',
    'DEVELOPMENT.md',
    'SETUP.md',
    'INSTALL.md',
    'USAGE.md',
    'API.md',
    'CHANGELOG.md',
]


def _fetch_github_api(api_url: str) -> dict | None:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ GitHub API.

    Args:
        api_url: URL —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ GitHub API.

    Returns:
        JSON –æ—Ç–≤–µ—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    headers = {
        'Accept': 'application/vnd.github.v3+json',
        'User-Agent': USER_AGENT,
    }

    try:
        response = requests.get(api_url, headers=headers, timeout=TIMEOUT_SECONDS)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f'‚ö†Ô∏è –û—à–∏–±–∫–∞ GitHub API: {str(e)}')
        return None


def _find_markdown_files(owner: str, repo: str) -> list[dict]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –≤–∞–∂–Ω—ã–µ markdown —Ñ–∞–π–ª—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏.

    Args:
        owner: –í–ª–∞–¥–µ–ª–µ—Ü —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.
        repo: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö: [{'name': ..., 'path': ..., 'url': ...}, ...]
    """
    found_files = []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    root_api_url = f'https://api.github.com/repos/{owner}/{repo}/contents'
    root_contents = _fetch_github_api(root_api_url)

    if root_contents and isinstance(root_contents, list):
        for item in root_contents:
            if item.get('type') == 'file' and item.get('name', '').upper() in [f.upper() for f in IMPORTANT_MD_FILES]:
                found_files.append({
                    'name': item['name'],
                    'path': item['path'],
                    'download_url': item.get('download_url'),
                })

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É docs/
    docs_api_url = f'https://api.github.com/repos/{owner}/{repo}/contents/docs'
    docs_contents = _fetch_github_api(docs_api_url)

    if docs_contents and isinstance(docs_contents, list):
        for item in docs_contents:
            if item.get('type') == 'file' and item.get('name', '').endswith('.md'):
                found_files.append({
                    'name': item['name'],
                    'path': item['path'],
                    'download_url': item.get('download_url'),
                })

    return found_files


def _fetch_file_content(download_url: str) -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –ø–æ download_url.

    Args:
        download_url: URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞.

    Returns:
        –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    try:
        response = requests.get(download_url, timeout=TIMEOUT_SECONDS)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {download_url}: {str(e)}')
        return ''


def _combine_markdown_content(files_data: list[dict]) -> str:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö markdown —Ñ–∞–π–ª–æ–≤.

    Args:
        files_data: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö.

    Returns:
        –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤.
    """
    combined = []

    for file_info in files_data:
        content = _fetch_file_content(file_info['download_url'])
        if content:
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞
            combined.append(f"\n{'=' * 60}\n–§–ê–ô–õ: {file_info['path']}\n{'=' * 60}\n")
            combined.append(content)

    return '\n'.join(combined)


def _parse_github(url: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å GitHub, —Å–æ–±–∏—Ä–∞—è –≤—Å–µ –≤–∞–∂–Ω—ã–µ markdown —Ñ–∞–π–ª—ã.

    –ò–∑–≤–ª–µ–∫–∞–µ—Ç: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–µ–ø–æ, –≤–ª–∞–¥–µ–ª–µ—Ü, –æ–ø–∏—Å–∞–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ README –∏ –¥—Ä—É–≥–∏—Ö .md —Ñ–∞–π–ª–æ–≤,
    –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–≤—ë–∑–¥, –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫.

    Args:
        url: URL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –Ω–∞ GitHub.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏: url, source, title, author, description,
        stars, language, content, content_length, files (—Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤).
        –ü—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª–µ–º 'error'.
    """
    # –ò–∑–≤–ª–µ–∫–∞–µ–º owner/repo –∏–∑ URL
    match = re.search(r'github\.com/([^/]+)/([^/]+)', url)
    if not match:
        return {'error': '–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL GitHub. –û–∂–∏–¥–∞–µ—Ç—Å—è: github.com/owner/repo'}

    owner, repo = match.groups()
    repo = repo.rstrip('/')

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π URL —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
    repo_url = f'https://github.com/{owner}/{repo}'
    print(f'üì• –ó–∞–≥—Ä—É–∂–∞—é GitHub: {repo_url}')

    soup, error = _fetch_page(repo_url)
    if error:
        return error

    # –û–ø–∏—Å–∞–Ω–∏–µ (–∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º)
    description = ''
    desc_elem = soup.find('p', class_='f4')
    if desc_elem:
        description = desc_elem.get_text(strip=True)

    # –ò—â–µ–º –≤—Å–µ –≤–∞–∂–Ω—ã–µ markdown —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ API
    print(f'üîç –ü–æ–∏—Å–∫ markdown —Ñ–∞–π–ª–æ–≤ –≤ {owner}/{repo}...')
    markdown_files = _find_markdown_files(owner, repo)

    if markdown_files:
        print(f'üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(markdown_files)}')
        for file in markdown_files:
            print(f'   ‚Ä¢ {file["path"]}')

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        combined_content = _combine_markdown_content(markdown_files)

        # –û–±—Ä–µ–∑–∞–µ–º, –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        combined_content = _truncate_content(combined_content, max_length=MAX_CONTENT_LENGTH * 2)
    else:
        # –ï—Å–ª–∏ API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–± (–ø–∞—Ä—Å–∏–Ω–≥ HTML)
        print('‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ API, –∏—Å–ø–æ–ª—å–∑—É—é –ø–∞—Ä—Å–∏–Ω–≥ HTML')
        combined_content = _extract_github_readme(soup)
        markdown_files = [{'name': 'README.md', 'path': 'README.md'}]

    return {
        'url': repo_url,
        'source': 'github',
        'title': f'{owner}/{repo}',
        'author': owner,
        'description': description,
        'stars': _extract_github_stars(soup),
        'language': _extract_github_language(soup),
        'content': combined_content,
        'content_length': len(combined_content),
        'files': [f['path'] for f in markdown_files],
    }


def _extract_github_stars(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–≤—ë–∑–¥ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è."""
    star_elem = soup.find('a', href=lambda x: x and '/stargazers' in x)
    if star_elem:
        star_text = star_elem.get_text(strip=True)
        numbers = re.findall(r'[\d,.]+[kK]?', star_text)
        if numbers:
            return numbers[0]
    return '0'


def _extract_github_language(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è."""
    lang_elem = soup.find(
        'span',
        class_='color-fg-default',
        attrs={'itemprop': 'programmingLanguage'},
    )
    if lang_elem:
        return lang_elem.get_text(strip=True)
    return '–ù–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω'


def _extract_github_readme(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ README."""
    readme_elem = soup.find('article', class_='markdown-body')

    if not readme_elem:
        return 'README –Ω–µ –Ω–∞–π–¥–µ–Ω'

    # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
    for tag in readme_elem(['script', 'style', 'svg', 'img']):
        tag.decompose()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
    content = readme_elem.get_text(separator='\n', strip=True)

    # –û—á–∏—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
    content = re.sub(r'\n{3,}', '\n\n', content)

    return _truncate_content(content)


# =============================================================================
# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# =============================================================================


def _run_tests() -> None:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Å—Ç—ã –ø–∞—Ä—Å–µ—Ä–æ–≤."""
    test_urls = [
        'https://habr.com/ru/articles/984968/',
        'https://github.com/anthropics/anthropic-cookbook',
        'https://infostart.ru/public/886103/',
    ]

    for url in test_urls:
        print(f'\n{"=" * 60}')
        print(f'–¢–ï–°–¢: {url}')
        print('=' * 60)

        result = get_article(url)

        if 'error' in result:
            print(f'‚ùå –û—à–∏–±–∫–∞: {result["error"]}')
        else:
            print(f'‚úÖ –ò—Å—Ç–æ—á–Ω–∏–∫: {result["source"]}')
            print(f'   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {result["title"]}')
            print(f'   –ê–≤—Ç–æ—Ä: {result["author"]}')
            print(f'   –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {result["content_length"]} —Å–∏–º–≤–æ–ª–æ–≤')
            print(f'\n   –ü—Ä–µ–≤—å—é (200 —Å–∏–º–≤–æ–ª–æ–≤):')
            print(f'   {result["content"][:200]}...')


if __name__ == '__main__':
    _run_tests()