# Study Agent

Telegram-бот для изучения статей из фиксированного набора источников. Бот парсит статьи, создаёт конспекты с помощью LLM и сохраняет результаты в базу данных.

## Возможности

- Парсинг статей из нескольких источников
- Генерация конспектов через локальные и облачные LLM
- Кэширование обработанных статей в SQLite
- Telegram-интерфейс для удобного взаимодействия

## Поддерживаемые источники

| Источник | Описание |
|----------|----------|
| habr.com | Технические статьи |
| github.com | README и документация репозиториев |
| infostart.ru | Статьи и публикации по 1С |

## Поддерживаемые модели

| Модель | Провайдер | Примечание |
|--------|-----------|------------|
| gemma3:12b | Ollama | По умолчанию, локальная |
| gpt-3.5-turbo | OpenAI | Облачная |
| gpt-4 | OpenAI | Облачная |

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/FrowningMonk/study_agent.git
cd study_agent
```

2. Создайте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Создайте файл `.env`:
```env
TELEGRAM_BOT_TOKEN=ваш_токен_от_BotFather
OPENAI_API_KEY=ваш_ключ_openai  # опционально
```

5. (Опционально) Установите [Ollama](https://ollama.ai/) для локальных моделей:
```bash
ollama pull gemma3:12b
```

## Запуск

### Telegram-бот
```bash
python bot.py
```

### CLI-режим
```bash
# С моделью по умолчанию
python pipeline.py https://habr.com/ru/articles/123456/

# С указанием модели
python pipeline.py https://habr.com/ru/articles/123456/ gpt-4

# Интерактивный режим
python pipeline.py
```

## Команды бота

| Команда | Описание |
|---------|----------|
| `/start` | Приветствие и инструкция |
| `/help` | Справка по использованию |
| `/model` | Выбор модели для генерации |
| `<URL>` | Отправить ссылку — получить конспект |

## Структура проекта

```
study_agent/
├── bot.py          # Telegram-бот
├── scraper.py      # Парсеры для источников
├── summarizer.py   # Генерация конспектов (OpenAI/Ollama)
├── pipeline.py     # Связующий пайплайн
├── database.py     # Работа с SQLite
├── requirements.txt
├── .env            # Конфигурация (не в репозитории)
├── data/           # БД и распарсенные статьи
└── conspect/       # Сохранённые конспекты
```

## Как это работает

1. Пользователь отправляет URL в бота
2. `scraper.py` определяет источник и извлекает контент
3. `database.py` проверяет, была ли статья обработана ранее
4. `summarizer.py` генерирует конспект через выбранную LLM
5. Результат сохраняется в БД и отправляется пользователю

